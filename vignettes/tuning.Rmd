---
title: "Model Tuning"
author: "Christopher Conley, Pei Wang, Jie Peng"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Model Tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


## Tuning Strategy for Managing Computational Constraints

The three tuning parameters in Spacemap provides a rich framework for discovering conditional dependencies between variables in high dimensional biological assays; however, exploring a large tuning grid can be very computationally demanding. In this tutorial we provided a strategy for finding the neighborhood of good tuning parameters given computational limits. The strategy finds suitiable neighborhoods of each tuning parameter by:

1. Tuning `space.joint` on $Y$ input data to find a neighborhood for the first tuning parameter, denoted as `slasso`. 
2. Tuning `spacemap` with the neigbhorhood of `slasso` from step 1, and exploring a broad range of values for `rlasso` and `rgroup`.  
3. Repeat step 2 if further tuning refinement is needed. 

We will illustrate the details of this strategy with an example from a simulation. The known truth of the simulation affords an evaluation addressing whether this strategy leads to reasonably tuned parameter selection. 

### Simulation Details

Load network simulation 1 which has topology that mimics a genetic regulatory network. The simulation was generated under a multivariate normal assumption with 171 response variables with 14 predictor variables (i.e. $Y$ = response, $X$ = predictors). There are 10 predictors that have no `X->Y` edges, 2 predictors with  13 `X->Y` edges and 2 predictors with 14 `X->Y` edges. Among the response, there are 2 hub variables with degree 12 and 13, but the degree of the other response variables does not exceed 4. There are two disconnected components in the graph of size 94 and 81. The data has been standardized (mean-centered with unit variance) for all variables. 

```{r}
library(spacemap)
data(sim1)
```

Extract data objects and report dimensions and sample size.

```{r}
dat <- sim1[c("XY", "X", "Y", "Xindex", "Yindex")]
N <- nrow(dat$X)
P <- ncol(dat$X)
Q <- ncol(dat$Y)
```

Extract the true partial correlation values for `X->Y` and `Y--Y` edges. Each non-zero value corresponds to a true edge. 

```{r}
trueParCor <- sim1$trueParCor
str(trueParCor)
```

## Tuning Example 

Tuning will be much faster if parallel computation is leveraged. Set up a parallel backend, in this case for a multicore machine. This code will use all available cores minus 1. 

```{r}
dopar <- TRUE
if (dopar) { 
  suppressPackageStartupMessages(library(doParallel))
  suppressPackageStartupMessages(library(parallel))
  ncores <- detectCores()  - 1
  cl <- makeCluster(ncores)
  registerDoParallel(cl)
}
```

## Find a neighborhood for `slasso`

Tune the `slasso` parameter by fitting the `space.joint` function from the SPACE model to $Y$ over a one-dimensional tuning grid. In the current implementation, the tuning parameter scales with the sample size. To find the right tuning scale, use a result from Meinshausen and Buhlmann (2006) which applies when when all `Y` have been standardized to have unit variance. The value of `alpha` is meant to control the false discovery rate; in our experience with this result, `alpha` should be set very conservatively to obtain an initial value closer to the CV-selected `slasso`. In our case, we set `alpha = 1e-5`. 

```{r}
lam1start <- function(n, q, alpha) { 
  sqrt(n) * qnorm(1 - (alpha/ (2*q^2)))
}
lam0 <- lam1start(n = floor(N - N*.10), q = Q, alpha = 1e-5)
lam0
```

Take the initial grid search to range from  [80% of `lam0`, 120% of `lam0`]. 

```{r}
#neighborhood parameter about lam0. 
eps1 <- 0.8
#initial grid size. 
ngrid <- 4
#grid
tsp <- expand.grid(slasso = seq(lam0*eps1, lam0*(1 + (1 - eps1)), length = ngrid))
summary(tsp)
```


In preparation of 10-K cross validation, we encourage the user to determine the split of the data since the data may have some special underlying population structure that needs to be balanced across the hold-out sets. 

```{r}
#for generating cross-validation folds
suppressPackageStartupMessages(library(caret))
#sample size
N <- nrow(dat$X)
#number of folds
K <- 10L
set.seed(265616L)
#no special population structure, but create randomized dummy structure of A and B
testSets <- createFolds(y = sample(x = c("A", "B"), size = N, replace = TRUE), k = K)
trainSets <- lapply(testSets, function(s) setdiff(seq_len(N), s))
```

The convergence of the algorithm depends on several input parameters, these include the convergence tolerance, the maximum number of iterations, 
the number of iterations 

```{r}
#convergence tolerance
tol <- 1e-6
#max. numbr of iterations for coordinate descent optimation
cd_iter <- 1e7
#number of iterations for estimating diagonal of concentration matrix.
iter <- 3
#ridge parameter for space (for elastic net if desired)
sridge <- 0
#ridge parameter for refit
refitRidge = 0.0
#do not standardize data internally since data has been standardized
iscale = FALSE
d01 <- tempdir()
```

```{r}
datyy <- dat
datyy$XY <- dat$Y
datyy$Yindex <- seq_len(Q)
datyy$Xindex <- seq_len(Q)
#undebug(crossValidation)
library(Matrix)
tictoc <- system.time({cvSpaceYY <- spacemap::crossValidation(Y.m = dat$Y, trainIds = trainSets, testIds = testSets, 
                                                              method = "space", tuneGrid = tsp)}) 
message("The 10-fold CV took ", round(tictoc[3]/60,2), " minutes")
```

We obtain a CV-error curve aross the tuning grid. 

```{r}
cvVis <- spacemap::tuneVis(cvOut = cvSpaceYY, testSetLen = sapply(testSets, length), 
                      tuneParam1 = tsp$slasso, tuneParam1Name = "slasso")
grid.arrange(cvVis[[1]], cvVis[[2]],cvVis[[3]],cvVis[[4]])
```

## Find a neighborhood for `rlasso` and `rgroup`


```{r}
tmap <- expand.grid(slasso = seq(65, 75, length = 2), 
                    rlasso = seq(24, 34, length = 2), 
                    rgroup = seq(5, 30, length = 2))
d02 <- tempdir()
```


```{r}
tictoc <- system.time({cvSpmap <- spacemap::crossValidation(Y.m = dat$Y, X.m = dat$X, trainIds = trainSets, testIds = testSets, 
                                                              method = "spacemap", tuneGrid = tmap)}) 

message("The 10-fold CV took ", round(tictoc[3]/60,2), " minutes")
#cvSpmap$minTune
```


Visualize the tuning parameters effect on the degrees of freedom and the loss function. 

```{r}
cvVis1 <- spacemap::tuneVis(cvOut = cvSpmap, testSetLen = sapply(testSets, length), 
                            tuneParam1 = tmap$slasso, tuneParam1Name = "slasso")
cvVis2 <- spacemap::tuneVis(cvOut = cvSpmap, testSetLen = sapply(testSets, length), 
                            tuneParam1 = tmap$rlasso, tuneParam1Name = "rlasso")
cvVis3 <- spacemap::tuneVis(cvOut = cvSpmap, testSetLen = sapply(testSets, length), 
                            tuneParam1 = tmap$rgroup, tuneParam1Name = "rgroup")
library(gridExtra)
grid.arrange(cvVis1[[1]], cvVis2[[1]],cvVis3[[1]],cvVis3[[4]])
```


```{r}
Perf <- list()
#undebug(cvPerf)
Perf$spmap <- spacemap::cvPerf(cvOut = cvSpmap, trueParCor = sim1$trueParCor, method = "spacemap")[1:8]
Perf$space <- spacemap::cvPerf(cvOut = cvSpaceYY, trueParCor = sim1$trueParCor, conditional = FALSE, method = "space")
Perf$space
#votePerf(Perf[c("space", "spmap")])
```

